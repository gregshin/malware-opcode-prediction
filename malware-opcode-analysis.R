# import libraries
library(tidyverse)
library(dplyr)
library(data.table)
library(mltools)
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(caTools)
library(randomForest)
library(e1071)
library(pROC)
library(ipred)
library(mlr)

set.seed(123)

# import data
df <- read_csv('data/all_data.csv')

# cleaning - remove ordinal data
df <- select(df, -c("File Name"))

# cleaning - edit column names for easier manipulation
colnames(df)[2] = "totalOpcodes"
colnames(df)[1] = "family"
names(df) <- gsub(x = names(df), pattern = ": ", replacement = "")

# cleaning - drop incomplete cases
df <- df[complete.cases(df),]

# data visualization
p <- ggplot(data=df, aes(x=family)) + geom_bar()

# drop HOTBAR because too few observations
df <- df %>% filter(family != 'HOTBAR')

# correlation analysis
corData <- df
corData$family <- as.factor(corData$family)
corFamily <- corData$family
corData <- select(corData, -c("family"))

corData <- as.data.table(corData)

# corData <- one_hot(corData)

corChart <- cor(corData, method=c("pearson"))

# get vector of correlations
corResults <- findCorrelation(corChart, cutoff = 0.8, exact = TRUE)

colnames(corChart)[corResults]

# adjust vector for dataset without one hot encoding
corResultsAdjusted <- corResults + 1

# drop correlated columns
df <- df[-c(corResultsAdjusted)]

# PCA analysis
df_pca <- prcomp(df[c(2:1007)], center = TRUE, scale = TRUE)

# screeplot of PCAs
screeplot(df_pca, type = "l", npcs = 15, main = "Screeplot of the first 15 PCs")

pcacumm <- cumsum(df_pca$sdev^2 / sum(df_pca$sdev^2))

plot(pcacumm[1:1000], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")

# using first 800 components as it accounts for over 95% of variance
pcacumm[860]

df_selected <- as.data.frame(df_pca$x[,1:860])

df_selected$family <- as.factor(df$family)

# split into training and testing
df_selected$sample <- sample.split(df_selected$family, SplitRatio = 0.8)

dataTrain <- subset(df_selected, sample == TRUE)
dataTest <- subset(dfselected, sample == FALSE)

## remove the sample column
dataTrain <- select(dataTrain, -c('sample'))
dataTest <- select(dataTest, -c('sample'))

dataTrain$family <- as.factor(dataTrain$family)
dataTest$family <- as.factor(dataTest$family)

## create simple decision tree
dTreeModel <- rpart(family ~ ., data = dataTrain, method = "class")
rpart.plot(dTreeModel, extra='auto')
dTreeModel

## prediction
dTreePredict <- predict(dTreeModel, dataTest, type="class")
dTreeAcc <- table(dataTest$family, dTreePredict)
print(dTreeAcc)

dAccuracy <- sum(diag(dTreeAcc)) / sum(dTreeAcc)
print(dAccuracy)

confusionMatrix(dTreePredict, dataTest$family)

## decision tree AUC
dTreePredProb <- predict(dTreeModel, newdata = dataTest, type="prob")
dTreeRoc <- multiclass.roc(dataTest$family, dTreePredProb)
auc(dTreeRoc)

# Naive Bayes
nbVanilla <- caret::train(family ~ ., data = dataTrain, method = "naive_bayes")
summary(nbVanilla)

nbVanPredict <- predict(nbVanilla, newdata = dataTest, type="raw")

confusionMatrix(nbVanPredict, dataTest$family)

nbVanProb <- predict(nbVanilla, newdata = dataTest, type="prob")
nbVanRoc <- multiclass.roc(dataTest$family, nbVanProb)
auc(nbVanRoc)

# SVM
svmModel <- svm(family ~ ., data = dataTrain, kernal = "radial", cost = 20, scale = FALSE, probability = TRUE)
svmPredict <- predict(svmModel, newdata = dataTest, type = "raw")

confusionMatrix(svmPredict, dataTest$family)

svmProb <- predict(svmModel, newdata = dataTest, probability = TRUE)
svmRoc <- multiclass.roc(dataTest$family, attr(svmProb, "probabilities"))
auc(svmRoc)

# ensemble - bagging
treeBag <- bagging(formula = family ~ ., data = dataTrain, nbagg = 100, coob = TRUE)
treeBagPredict <- predict(treeBag, dataTest, type="class")

confusionMatrix(dataTest$family, treeBagPredict)

treeBagPredProb <- predict(treeBag, dataTest, type="prob")

treeBagRoc <- multiclass.roc(dataTest$family, treeBagPredProb)
auc(treeBagRoc)

# Naive Bayes model with 10x Cross Validation

nbModel <- caret::train(family ~ ., data=dataTrain, trControl=trainControl(method="cv", number=10), method="naive_bayes")
nbCVPredict <- predict(nbModel, newdata = dataTest, type="raw")

confusionMatrix(dataTest$family, nbCVPredict)

nbCVPredProb <- predict(nbModel, newdata = dataTest, type="prob")
nbCVRoc <- multiclass.roc(dataTest$family, nbCVPredProb)
auc(nbCVRoc)

# ensemble - input related - random forest
rfModel <- randomForest(family ~ ., data = dataTrain, ntree = 500, importance = TRUE)
rfPredict <- predict(rfModel, dataTest, type="class")

confusionMatrix(rfPredict, dataTest$family)

getTree(rfModel, k = 1, labelVar = TRUE)
getTree(rfModel, k = 2, labelVar = TRUE)
getTree(rfModel, k = 3, labelVar = TRUE)

plot(getTree(rfModel, k = 1, labelVar = TRUE))

plot(rfModel)

rfPredProb <- predict(rfModel, dataTest, type="prob")

rfRoc <- multiclass.roc(dataTest$family, rfPredProb)
auc(rfRoc)

# ensemble - output related - ecoc
task <- makeClassifTask(data = dataTrain, target = "family")
treeLearner <- makeLearner("classif.rpart")
treeMulticlass <- makeMulticlassWrapper(treeLearner)
ecocModel <- mlr::train(treeMulticlass, task)
ecocPredict <- predict(ecocModel, newdata=dataTest, type="class")

confusionMatrix(ecocPredict$data$truth, ecocPredict$data$response)

treeLearnerProb <- makeLearner("classif.rpart", predict.type = "prob")